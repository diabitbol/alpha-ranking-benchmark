{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366aff5b-91cd-4af2-8e81-8f20add55892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8636e31-7ebc-4cea-b89d-8afc358e8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../sp500_ohlcv_2005_2025_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d065e-8c35-4139-9994-0f0ff9494933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose df a les colonnes : Date, Ticker, Open, High, Low, Close, Volume\n",
    "df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "\n",
    "# --- 1) Log-return\n",
    "df[\"log_return\"] = np.log(df[\"Close\"]) - np.log(df[\"Close\"].shift(1))\n",
    "\n",
    "# --- 2) Target direction (r_{t+1} > 0)\n",
    "df[\"target\"] = (df.groupby(\"Ticker\")[\"log_return\"].shift(-1) > 0).astype(int)\n",
    "\n",
    "# --- 3) Features momentum\n",
    "df[\"mom_5\"]  = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x / x.shift(5) - 1)\n",
    "df[\"mom_21\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x / x.shift(21) - 1)\n",
    "\n",
    "# --- 4) Features volatilité\n",
    "df[\"vol_5\"]  = df.groupby(\"Ticker\")[\"log_return\"].transform(lambda x: x.rolling(5).std())\n",
    "df[\"vol_21\"] = df.groupby(\"Ticker\")[\"log_return\"].transform(lambda x: x.rolling(21).std())\n",
    "\n",
    "# --- 5) High–Low range\n",
    "df[\"range\"] = (df[\"High\"] - df[\"Low\"]) / df[\"Open\"]\n",
    "\n",
    "# --- 6) Volume z-score\n",
    "df[\"volume_z\"] = df.groupby(\"Ticker\")[\"Volume\"].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "df['Return'] = df.groupby('Ticker')['Close'].pct_change(fill_method=None)\n",
    "\n",
    "# --- 7) Clean\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940909dc-58fd-45fe-9301-8e7a870d2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "resp.raise_for_status()  # lève une erreur si 4xx/5xx\n",
    "\n",
    "tables = pd.read_html(resp.text, header=0)\n",
    "sp500 = tables[0]\n",
    "\n",
    "sp500 = sp500.rename(columns={\"Symbol\": \"Ticker\", \"GICS Sector\": \"Sector\"})\n",
    "sp500['Ticker'] = sp500['Ticker'].str.replace('.', '-', regex=False)\n",
    "\n",
    "df = df.sort_values(by=['Ticker', 'Date'])\n",
    "\n",
    "df = df.merge(\n",
    "    sp500[['Ticker', 'Sector']], \n",
    "    on='Ticker', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8530e-39e0-43c5-8fa7-e606bae87bcf",
   "metadata": {},
   "source": [
    "la variable Sector est catégorielle par conséquent on va l'encoder par par la volatilité moyenne du secteur. Ce choix est motivé par la raison suivante. On veut faire comprendre au modèle que certains secteurs sont très volatils et très dépendants de chocs extérieur ainsi pour ces secteurs la, l'évolution futur de l'action est moins évidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eeb5fe-ae7b-4325-922b-40b1b663205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Calcul de volatilité moyenne par secteur\n",
    "sector_vol_mean = df.groupby(\"Sector\")[\"vol_21\"].mean()\n",
    "\n",
    "# 2) Encodage de Sector par cette moyenne\n",
    "df[\"Sector_encoded\"] = df[\"Sector\"].map(sector_vol_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc960f06-bac8-409b-8a61-8424f1786a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "def regression(df, features):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    train = df[df[\"Date\"] < \"2018-01-01\"]\n",
    "    val   = df[(df[\"Date\"] >= \"2018-01-01\") & (df[\"Date\"] < \"2021-01-01\")]\n",
    "    test  = df[df[\"Date\"] >= \"2021-01-01\"].copy()   # copy important\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "\n",
    "    # --- Autres métriques globales\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    return  model, sector_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226df3a-a957-4d54-b03f-c582a7f4cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression(df, [\"mom_5\", \"mom_21\", \"vol_5\", \"vol_21\", \"range\", \"volume_z\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0fb1b4-12e1-41e9-8e6e-4ad976f6b55c",
   "metadata": {},
   "source": [
    "De premier abord, la regression parait ne pas être suffisante pour prédire le signe du rendement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66424cc-4463-4918-a7c1-40bdc908433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def training(df, features):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    train = df[df[\"Date\"] < \"2018-01-01\"]\n",
    "    val   = df[(df[\"Date\"] >= \"2018-01-01\") & (df[\"Date\"] < \"2021-01-01\")]\n",
    "    test  = df[df[\"Date\"] >= \"2021-01-01\"].copy()   # copy important\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    # --- 9) Modèle XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=800,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "\n",
    "    # --- Autres métriques globales\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "    # print(\"F1        :\", f1_score(y_test, y_pred))\n",
    "    # print(\"AUC       :\", roc_auc_score(y_test, y_pred_proba))\n",
    "    # print()\n",
    "    # print(confusion_matrix(y_test, y_pred))\n",
    "    # print()\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "\n",
    "     # Ajout des prédictions dans test\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    return  sector_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea988d9-422c-4945-8cd3-b7b8552965f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training(df, [\"Close\", \"mom_5\", \"mom_21\", \"vol_5\", \"vol_21\", \"range\", \"volume_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a249cc-3d44-4496-9cd0-decd08407e9d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"mom_21\", \"vol_5\", \"vol_21\", \"range\", \"volume_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab465bb6-61a1-4209-b599-5a12fe874a56",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"vol_21\", \"range\", \"volume_z\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2aada-14ad-44ae-a8ff-46fbea9a5c26",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668c191-49c7-4555-9993-b2e2139f1e2b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"Return\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0dd6f-eebe-4905-af7e-994aec23e312",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"Sector_encoded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55c171-8eab-4c80-9768-8a3a31af0a2c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"volume_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf461b9-e903-441c-b7a0-96f2e506367c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"Open\", \"Close\", \"High\", \"Low\", \"Volume\", \"mom_5\", \"mom_21\", \"vol_5\", \"vol_21\", \"range\", \"volume_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e56db-63e2-4b2f-960b-3b050373d7f6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31426f00-5202-412a-926b-162c800d5d4a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"range\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22fe66-949e-4db8-b17b-cbde0f571846",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training(df, [\"mom_5\", \"vol_5\", \"range\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559d6e7-1420-4a83-8dad-4ad27af0f007",
   "metadata": {},
   "source": [
    "meme en ajoutant des variable pertinentes, l'accuracy reste sensiblement la meme peu importe le secteur. L'accuracy max réalisé est 0.5167. Peut etre qu'étant donné le fait que les années d'entrainement sont très anciennes par rapport aux années de test le modèle est mal adapté aux nouvelles années\n",
    "\n",
    "Changeons de méthode d'entrainement : éparpillons des années de test et de validation dans tous le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84424a1f-6964-4485-a24b-a7928c649644",
   "metadata": {},
   "source": [
    "# Ajout de la variable secteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5297153-47bc-467e-b552-328f48b2055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def training2(df, features):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    mask_train = (\n",
    "        df[\"Date\"].between(\"2004-01-01\", \"2008-01-01\") |\n",
    "        df[\"Date\"].between(\"2010-01-01\", \"2013-01-01\") |\n",
    "        df[\"Date\"].between(\"2015-01-01\", \"2018-01-01\") |\n",
    "        df[\"Date\"].between(\"2020-01-01\", \"2023-01-01\")\n",
    "    )\n",
    "\n",
    "    mask_val = (\n",
    "        df[\"Date\"].between(\"2008-01-01\", \"2009-01-01\") |\n",
    "        df[\"Date\"].between(\"2013-01-01\", \"2014-01-01\") |\n",
    "        df[\"Date\"].between(\"2018-01-01\", \"2019-01-01\") |\n",
    "        df[\"Date\"].between(\"2023-01-01\", \"2024-01-01\")\n",
    "    )\n",
    "\n",
    "    mask_test = (\n",
    "        df[\"Date\"].between(\"2009-01-01\", \"2010-01-01\") |\n",
    "        df[\"Date\"].between(\"2014-01-01\", \"2015-01-01\") |\n",
    "        df[\"Date\"].between(\"2019-01-01\", \"2020-01-01\") |\n",
    "        df[\"Date\"].between(\"2024-01-01\", \"2025-01-01\")\n",
    "    )\n",
    "\n",
    "    train = df[mask_train]\n",
    "    val   = df[mask_val]\n",
    "    test  = df[mask_test].copy()   # copy pour pouvoir ajouter des colonnes\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    # --- 9) Modèle XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=800,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Ajout des prédictions dans test\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return  sector_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4301c-1bd7-46d1-8b4b-585e8515aa4d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training2(df, [\"mom_21\", \"vol_21\", \"range\", \"volume_z\", \"Sector_encoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a81a0-f910-44a6-b356-da3547774258",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training2(df, [\"mom_5\", \"vol_5\", \"volume_z\", \"Sector_encoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07df931-0b27-4ffa-bb66-b8547c4b1fce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training2(df, [\"mom_5\", \"vol_5\", \"volume_z\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e8d57-8684-48cb-be9d-1824f7d830f3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training2(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91cf75f-8d4d-4cfd-afd4-829c36abc3d7",
   "metadata": {},
   "source": [
    "la meilleur accuracy est meilleure que celle obtenue précédemment 0.5293 mais reste cependant faible. \n",
    "\n",
    "Essayons de préciser nos modèles en les concentrant sur un secteur. Peut être que les modèles précédent essayaient de trop généraliser ce qui les rendait imprécis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba9f99-93e6-4865-a593-2696e70bf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def training3(df, features, sector):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    mask_train = (\n",
    "        (df[\"Sector\"] == sector) &\n",
    "        (df[\"Date\"].between(\"2004-01-01\", \"2008-01-01\") |\n",
    "        df[\"Date\"].between(\"2010-01-01\", \"2013-01-01\") |\n",
    "        df[\"Date\"].between(\"2015-01-01\", \"2018-01-01\") |\n",
    "        df[\"Date\"].between(\"2020-01-01\", \"2023-01-01\"))\n",
    "    )\n",
    "\n",
    "    mask_val = (\n",
    "        (df[\"Sector\"] == sector) &\n",
    "        (df[\"Date\"].between(\"2008-01-01\", \"2009-01-01\") |\n",
    "        df[\"Date\"].between(\"2013-01-01\", \"2014-01-01\") |\n",
    "        df[\"Date\"].between(\"2018-01-01\", \"2019-01-01\") |\n",
    "        df[\"Date\"].between(\"2023-01-01\", \"2024-01-01\"))\n",
    "    )\n",
    "\n",
    "    mask_test = (       \n",
    "        (df[\"Sector\"] == sector) &\n",
    "        (df[\"Date\"].between(\"2009-01-01\", \"2010-01-01\") |\n",
    "        df[\"Date\"].between(\"2014-01-01\", \"2015-01-01\") |\n",
    "        df[\"Date\"].between(\"2019-01-01\", \"2020-01-01\") |\n",
    "        df[\"Date\"].between(\"2024-01-01\", \"2025-01-01\"))\n",
    "    )\n",
    "\n",
    "    train = df[mask_train]\n",
    "    val   = df[mask_val]\n",
    "    test  = df[mask_test].copy()   # copy pour pouvoir ajouter des colonnes\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    # --- 9) Modèle XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=800,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Ajout des prédictions dans test\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return  sector_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848760d-6922-4a88-9ebf-357684babf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training3(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"Financials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f21ad9-07fb-4d44-a554-2f1394eef58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training3(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"Utilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf2cbc-7199-48ba-a306-1d173f4596ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training3(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"Information Technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b3193-cc0c-4f0d-9c17-36d1903d0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training4(df, features, ticker):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    mask_train = (\n",
    "        (df[\"Ticker\"] == ticker) &\n",
    "        (df[\"Date\"].between(\"2004-01-01\", \"2008-01-01\") |\n",
    "        df[\"Date\"].between(\"2010-01-01\", \"2013-01-01\") |\n",
    "        df[\"Date\"].between(\"2015-01-01\", \"2018-01-01\") |\n",
    "        df[\"Date\"].between(\"2020-01-01\", \"2023-01-01\"))\n",
    "    )\n",
    "\n",
    "    mask_val = (\n",
    "        (df[\"Ticker\"] == ticker) &\n",
    "        (df[\"Date\"].between(\"2008-01-01\", \"2009-01-01\") |\n",
    "        df[\"Date\"].between(\"2013-01-01\", \"2014-01-01\") |\n",
    "        df[\"Date\"].between(\"2018-01-01\", \"2019-01-01\") |\n",
    "        df[\"Date\"].between(\"2023-01-01\", \"2024-01-01\"))\n",
    "    )\n",
    "\n",
    "    mask_test = (       \n",
    "        (df[\"Ticker\"] == ticker) &\n",
    "        (df[\"Date\"].between(\"2009-01-01\", \"2010-01-01\") |\n",
    "        df[\"Date\"].between(\"2014-01-01\", \"2015-01-01\") |\n",
    "        df[\"Date\"].between(\"2019-01-01\", \"2020-01-01\") |\n",
    "        df[\"Date\"].between(\"2024-01-01\", \"2025-01-01\"))\n",
    "    )\n",
    "\n",
    "    train = df[mask_train]\n",
    "    val   = df[mask_val]\n",
    "    test  = df[mask_test].copy()   # copy pour pouvoir ajouter des colonnes\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    # --- 9) Modèle XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=800,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Ajout des prédictions dans test\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return  sector_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72797508-f942-455a-b9c8-b7f7ad60937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training4(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"GOOGL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d537e5a-9322-4050-9bf3-ca8f93e6576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training4(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"XOM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8bce59-71d7-4625-bec9-a7b402b728de",
   "metadata": {},
   "outputs": [],
   "source": [
    "training4(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"JPM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f0fb1-0185-4a05-9509-f02eafcca458",
   "metadata": {},
   "outputs": [],
   "source": [
    "training4(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"], \"PLD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c79e22-bd8c-42e1-b4d1-788cbded1a0d",
   "metadata": {},
   "source": [
    "On remarque que souvent la catégories la moins bien prédites est celle de l'énergie qui est le secteur le plus sensible au choc d'après ce qui a été vu précédemment. Peut être que le faible score provient des période de crises dues à des chocs extérieurs aux marché qui sont imprévisibles à l'aide 'uniquement les données. Essayons de se limiter à la période la plus calme de 2005-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97997238-b567-4ed8-aae2-0fb26866d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training5(df, features):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    mask_train = (\n",
    "        df[\"Date\"].between(\"2010-01-01\", \"2012-01-01\") \n",
    "    )\n",
    "\n",
    "    mask_val = (\n",
    "        df[\"Date\"].between(\"2012-01-01\", \"2013-01-01\") \n",
    "    )\n",
    "\n",
    "    mask_test = (\n",
    "        df[\"Date\"].between(\"2013-01-01\", \"2014-01-01\") \n",
    "    )\n",
    "\n",
    "    train = df[mask_train]\n",
    "    val   = df[mask_val]\n",
    "    test  = df[mask_test].copy()   # copy pour pouvoir ajouter des colonnes\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    # --- 9) Modèle XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=800,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Ajout des prédictions dans test\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return  sector_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441a272-c7c1-46d5-99bc-d4885a0305a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training5(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed447d3-f7b7-4400-8a42-d38e40cd23f0",
   "metadata": {},
   "source": [
    "Malheuresement les résultats ne sont pas sensiblement meilleur\n",
    "\n",
    " Or on a vu précédemment que ces périodes de crise peuvent être caractérisées par des période de volatilité élevée. Ainsi on va retirer du dataset tous les moments ou la volatilité est élevée statistiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054aa130-d9bf-46b1-8bc0-cbc1412ac315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr_per_ticker(df):\n",
    "    # Fonction appliquée à chaque groupe (chaque Ticker)\n",
    "    def filter_group(g):\n",
    "        q1 = g['vol_21'].quantile(0.25)\n",
    "        q3 = g['vol_21'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        upper = q3 + 3* iqr\n",
    "\n",
    "        # on garde seulement les lignes sous la borne haute\n",
    "        return g[g['vol_21'] <= upper]\n",
    "\n",
    "    # groupby puis concaténation automatique\n",
    "    return df.groupby('Ticker', group_keys=False).apply(filter_group, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db37c0-bf18-4eef-b9da-88986eb8ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training6(df, features):\n",
    "\n",
    "    # --- 8) Train/val/test split\n",
    "    mask_train = (\n",
    "        df[\"Date\"].between(\"2004-01-01\", \"2008-01-01\") |\n",
    "        df[\"Date\"].between(\"2010-01-01\", \"2013-01-01\") |\n",
    "        df[\"Date\"].between(\"2015-01-01\", \"2018-01-01\") |\n",
    "        df[\"Date\"].between(\"2020-01-01\", \"2023-01-01\")\n",
    "    )\n",
    "\n",
    "    mask_val = (\n",
    "        df[\"Date\"].between(\"2008-01-01\", \"2009-01-01\") |\n",
    "        df[\"Date\"].between(\"2013-01-01\", \"2014-01-01\") |\n",
    "        df[\"Date\"].between(\"2018-01-01\", \"2019-01-01\") |\n",
    "        df[\"Date\"].between(\"2023-01-01\", \"2024-01-01\")\n",
    "    )\n",
    "\n",
    "    mask_test = (\n",
    "        df[\"Date\"].between(\"2009-01-01\", \"2010-01-01\") |\n",
    "        df[\"Date\"].between(\"2014-01-01\", \"2015-01-01\") |\n",
    "        df[\"Date\"].between(\"2019-01-01\", \"2020-01-01\") |\n",
    "        df[\"Date\"].between(\"2024-01-01\", \"2025-01-01\")\n",
    "    )\n",
    "\n",
    "    train_uncleaned = df[mask_train]\n",
    "    val_uncleaned   = df[mask_val]\n",
    "    test_uncleaned  = df[mask_test].copy()   # copy pour pouvoir ajouter des colonnes\n",
    "\n",
    "    train = remove_outliers_iqr_per_ticker(train_uncleaned)\n",
    "    val = remove_outliers_iqr_per_ticker(val_uncleaned)\n",
    "    test = remove_outliers_iqr_per_ticker(test_uncleaned).copy()\n",
    "\n",
    "    X_train, y_train = train[features], train[\"target\"]\n",
    "    X_val, y_val     = val[features], val[\"target\"]\n",
    "    X_test, y_test   = test[features], test[\"target\"]\n",
    "    \n",
    "    # --- 9) Modèle XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=800,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions sur le test\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Ajout des prédictions dans test\n",
    "    test[\"pred\"] = y_pred\n",
    "\n",
    "    # --- Calcul F1 par secteur (ajuste \"Sector\" si ta colonne s'appelle autrement)\n",
    "    sector_f1 = (\n",
    "        test.groupby(\"Sector\")\n",
    "            .apply(lambda g: accuracy_score(g[\"target\"], g[\"pred\"]), include_groups=False)\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return  sector_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5004a1d-5ba9-42d6-a399-07c11ad7d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "training6(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\", \"vol_21\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b86a11-6447-46ce-b33f-5138dadfc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training6(df, [\"mom_5\", \"vol_5\", \"range\", \"volume_z\", \"mom_21\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d14de-1701-46cb-b005-8e20a3ad756f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Dans un premier temps, nous avons tenté de construire un modèle de machine learning capable de prédire le signe du rendement journalier des actions du S&P 500.\n",
    "Cette approche s’est révélée insuffisante : malgré l’utilisation de différents modèles (régression logistique, XGBoost) et d’un ensemble de variables explicatives dérivées du prix (momentum 5 et 21 jours, volatilité instantanée, amplitude intraday, volume standardisé), les performances prédictives sont restées très proches du hasard.\n",
    "\n",
    "Ce résultat n’est pas dû à un problème de modélisation ou de qualité des données : il reflète au contraire une propriété structurelle des rendements financiers quotidiens.\n",
    "\n",
    "En effet, la littérature empirique (Lo & MacKinlay, 1999 ; Bouchaud et al., 2003) montre que les rendements journaliers des actifs financiers sont extrêmement bruités et présentent :\n",
    "\n",
    "une autocorrélation quasi nulle, ce qui signifie que le rendement d’un jour ne contient presque aucune information exploitable pour prédire celui du lendemain ;\n",
    "\n",
    "une variance dominée par le bruit de marché, lui-même influencé par des facteurs non observables dans les données OHLCV (annonces macroéconomiques, surprises de résultats, flux d’ordres intraday, microstructure, sentiment, chocs exogènes, etc.) ;\n",
    "\n",
    "une prédictibilité directionnelle théorique très faible, généralement limitée à 2–4 % d’information exploitable, ce qui entraîne un plafond d’accuracy proche de 52–54 % même pour les modèles les plus puissants.\n",
    "\n",
    "Dans ce contexte, les variables utilisées — basées sur des caractéristiques de prix relativement lentes (momentum, volatilité historique, volume, amplitudes intraday) — sont structurellement peu informatives pour capturer le signal directionnel à un horizon aussi court. La faible performance observée est donc cohérente avec les limites théoriques du problème.\n",
    "\n",
    "En revanche, ces mêmes variables sont fortement liées à la dynamique de la volatilité, qui présente une autocorrélation élevée et des régimes persistants (volatility clustering). Contrairement aux rendements, la volatilité est un processus beaucoup plus régulier et prévisible, ce qui en fait une cible de modélisation beaucoup plus appropriée.\n",
    "\n",
    "C’est pourquoi nous avons choisi, dans un second temps, de réorienter notre travail vers la prédiction de la volatilité future, un problème mieux posé et pour lequel les méthodes d’apprentissage peuvent exploiter un véritable signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a4f99-73e6-4353-bfcd-6ff5fb3c5b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
