{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7aeb3db-375b-4ee4-a460-50b8dc5f68ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.66-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/python/lib/python3.13/site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/python/lib/python3.13/site-packages (from yfinance) (2.32.5)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.0.0 in /opt/python/lib/python3.13/site-packages (from yfinance) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/python/lib/python3.13/site-packages (from yfinance) (2025.2)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.6-py313-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.18.2.tar.gz (949 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.2/949.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /opt/python/lib/python3.13/site-packages (from yfinance) (4.14.0)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.13.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/python/lib/python3.13/site-packages (from yfinance) (6.32.1)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/python/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/python/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/python/lib/python3.13/site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /opt/python/lib/python3.13/site-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
      "Requirement already satisfied: pycparser in /opt/python/lib/python3.13/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.31->yfinance) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.31->yfinance) (2.5.0)\n",
      "Downloading yfinance-0.2.66-py2.py3-none-any.whl (123 kB)\n",
      "Downloading curl_cffi-0.13.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozendict-2.4.6-py313-none-any.whl (16 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Building wheels for collected packages: multitasking, peewee\n",
      "\u001b[33m  DEPRECATION: Building 'multitasking' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'multitasking'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for multitasking (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multitasking: filename=multitasking-0.0.12-py3-none-any.whl size=15636 sha256=7b222efaafb9b77d1fd42c26b5c83aa16c3ab42a083a5086e05de1facb3ca28c\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/1e/df/0f/e2bbb22d689b30c681feb5410ab64a2523437b34c8ecfc6476\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.18.2-cp313-cp313-linux_x86_64.whl size=1045008 sha256=109ab8f81522506f69bc9e9ed98a832f00ae9bd6f55c34bf0e9e1e9420a2aae4\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/1c/48/cc/00b7d0e7defa21a58915654917c89eaedd32a6e614d8e4ad92\n",
      "Successfully built multitasking peewee\n",
      "Installing collected packages: peewee, multitasking, websockets, frozendict, curl_cffi, yfinance\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [yfinance]4/6\u001b[0m [curl_cffi]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed curl_cffi-0.13.0 frozendict-2.4.6 multitasking-0.0.12 peewee-3.18.2 websockets-15.0.1 yfinance-0.2.66\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a79f72-eda8-467c-985b-7175c9406daf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6dfb41-c057-42b4-a99a-434af01c5e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_916/1169866939.py:26: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text, header=0)  # nécessite lxml OU html5lib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 tickers récupérés. Exemples: ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  503 of 503 completed\n",
      "\n",
      "46 Failed downloads:\n",
      "['CMI']: Timeout('Failed to perform, curl: (28) Operation timed out after 10025 milliseconds with 136211 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['PFE']: Timeout('Failed to perform, curl: (28) Operation timed out after 10413 milliseconds with 152534 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['PEP']: Timeout('Failed to perform, curl: (28) Operation timed out after 10034 milliseconds with 119067 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['RJF']: Timeout('Failed to perform, curl: (28) Operation timed out after 10746 milliseconds with 135967 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['EQR']: Timeout('Failed to perform, curl: (28) Operation timed out after 10343 milliseconds with 116475 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['COST']: Timeout('Failed to perform, curl: (28) Operation timed out after 10616 milliseconds with 135460 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['ICE']: Timeout('Failed to perform, curl: (28) Operation timed out after 10805 milliseconds with 135967 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['FAST']: Timeout('Failed to perform, curl: (28) Operation timed out after 11327 milliseconds with 134657 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['GRMN']: Timeout('Failed to perform, curl: (28) Operation timed out after 10915 milliseconds with 102175 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['AAPL']: Timeout('Failed to perform, curl: (28) Operation timed out after 11129 milliseconds with 152738 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['APD']: Timeout('Failed to perform, curl: (28) Operation timed out after 10963 milliseconds with 102596 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['EL']: Timeout('Failed to perform, curl: (28) Operation timed out after 10372 milliseconds with 118623 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['YUM']: Timeout('Failed to perform, curl: (28) Operation timed out after 11921 milliseconds with 169366 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['ADP']: Timeout('Failed to perform, curl: (28) Operation timed out after 10966 milliseconds with 135647 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['CPB']: Timeout('Failed to perform, curl: (28) Operation timed out after 10108 milliseconds with 118986 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['MSI']: Timeout('Failed to perform, curl: (28) Operation timed out after 11824 milliseconds with 117931 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['CAH']: Timeout('Failed to perform, curl: (28) Operation timed out after 11682 milliseconds with 100875 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['CAT']: Timeout('Failed to perform, curl: (28) Operation timed out after 10045 milliseconds with 101746 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['EMN']: Timeout('Failed to perform, curl: (28) Operation timed out after 10591 milliseconds with 100376 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['PRU']: Timeout('Failed to perform, curl: (28) Operation timed out after 10376 milliseconds with 100211 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['XOM']: Timeout('Failed to perform, curl: (28) Operation timed out after 10433 milliseconds with 118658 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['MRK']: Timeout('Failed to perform, curl: (28) Operation timed out after 12046 milliseconds with 135206 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['LEN']: Timeout('Failed to perform, curl: (28) Operation timed out after 12497 milliseconds with 169441 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['WST']: Timeout('Failed to perform, curl: (28) Operation timed out after 10257 milliseconds with 100129 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['TMO']: Timeout('Failed to perform, curl: (28) Operation timed out after 11462 milliseconds with 117488 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['LHX']: Timeout('Failed to perform, curl: (28) Operation timed out after 11643 milliseconds with 152398 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['RL']: Timeout('Failed to perform, curl: (28) Operation timed out after 11517 milliseconds with 85015 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['ERIE']: Timeout('Failed to perform, curl: (28) Operation timed out after 10018 milliseconds with 83225 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['WYNN']: Timeout('Failed to perform, curl: (28) Operation timed out after 11906 milliseconds with 134266 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['QCOM']: Timeout('Failed to perform, curl: (28) Operation timed out after 11654 milliseconds with 118546 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['MA']: Timeout('Failed to perform, curl: (28) Operation timed out after 10041 milliseconds with 66334 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['LOW']: Timeout('Failed to perform, curl: (28) Operation timed out after 10410 milliseconds with 117766 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['LVS']: Timeout('Failed to perform, curl: (28) Operation timed out after 11482 milliseconds with 101552 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['CTRA']: Timeout('Failed to perform, curl: (28) Operation timed out after 10201 milliseconds with 85015 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['MET']: Timeout('Failed to perform, curl: (28) Operation timed out after 10339 milliseconds with 85015 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['MCK']: Timeout('Failed to perform, curl: (28) Operation timed out after 10643 milliseconds with 84656 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['BAX']: Timeout('Failed to perform, curl: (28) Operation timed out after 12560 milliseconds with 119316 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['RVTY']: Timeout('Failed to perform, curl: (28) Operation timed out after 10348 milliseconds with 101906 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['SRE']: Timeout('Failed to perform, curl: (28) Operation timed out after 10921 milliseconds with 118997 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['TDG']: Timeout('Failed to perform, curl: (28) Operation timed out after 11167 milliseconds with 119211 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['SNA']: Timeout('Failed to perform, curl: (28) Operation timed out after 10110 milliseconds with 85006 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['AME']: Timeout('Failed to perform, curl: (28) Operation timed out after 10128 milliseconds with 101505 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['AOS']: Timeout('Failed to perform, curl: (28) Operation timed out after 10421 milliseconds with 135835 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['GWW']: Timeout('Failed to perform, curl: (28) Operation timed out after 10116 milliseconds with 135804 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['GOOGL']: Timeout('Failed to perform, curl: (28) Operation timed out after 10503 milliseconds with 170295 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['DD']: Timeout('Failed to perform, curl: (28) Operation timed out after 10383 milliseconds with 169516 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sauvegardé : sp500_ohlcv_2005_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "def get_sp500_tickers() -> list[str]:\n",
    "    # 1) Essai: liste intégrée yfinance (souvent suffit)\n",
    "    try:\n",
    "        tickers = yf.tickers_sp500()\n",
    "        if tickers:\n",
    "            return [t.replace('.', '-') for t in tickers]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Fallback: Wikipédia avec User-Agent pour éviter 403\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    r = requests.get(WIKI_URL, headers=headers, timeout=20)\n",
    "    r.raise_for_status()  # lèvera une erreur si 403/4xx/5xx\n",
    "    # Pandas peut lire depuis la chaîne HTML directement\n",
    "    tables = pd.read_html(r.text, header=0)  # nécessite lxml OU html5lib\n",
    "    sp500 = tables[0]\n",
    "    return sp500[\"Symbol\"].str.replace('.', '-', regex=False).tolist()\n",
    "\n",
    "def download_sp500_ohlcv_csv(start=\"2005-01-01\", end=\"2025-01-01\", out_csv=\"sp500_ohlcv_2005_2025.csv\"):\n",
    "    tickers = get_sp500_tickers()\n",
    "    print(f\"{len(tickers)} tickers récupérés. Exemples: {tickers[:10]}\")\n",
    "\n",
    "    data = yf.download(\n",
    "        tickers,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        interval=\"1d\",\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=True,   # ajuste splits/dividendes\n",
    "        threads=True,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    # Reformater en (Date, Ticker)\n",
    "    frames = []\n",
    "    for t in tickers:\n",
    "        if isinstance(data.columns, pd.MultiIndex) and t in data.columns.get_level_values(0):\n",
    "            df_t = data[t].copy()\n",
    "        else:\n",
    "            # cas rare: un seul ticker ou structure différente\n",
    "            if t == tickers[0]:\n",
    "                df_t = data.copy()\n",
    "            else:\n",
    "                continue\n",
    "        df_t[\"Ticker\"] = t\n",
    "        df_t = df_t.reset_index()\n",
    "        frames.append(df_t)\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df = df.set_index([\"Date\", \"Ticker\"]).sort_index()\n",
    "\n",
    "    # Garder colonnes standard si présentes\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adj Close\"] if c in df.columns]\n",
    "    df = df[keep]\n",
    "    # Si auto_adjust=True, Adj Close == Close → on peut la supprimer\n",
    "    if \"Adj Close\" in df.columns and \"Close\" in df.columns:\n",
    "        if df[\"Adj Close\"].equals(df[\"Close\"]):\n",
    "            df = df.drop(columns=[\"Adj Close\"])\n",
    "\n",
    "    df.to_csv(out_csv)\n",
    "    print(f\"✅ Sauvegardé : {out_csv}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_sp500_ohlcv_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fd04614-3f81-4599-93ed-81bff8e473d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_916/1170582321.py:28: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  sp500 = pd.read_html(html, header=0)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 tickers S&P 500 chargés. Exemples: ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A']\n",
      "[Batch 1/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 2/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 3/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 4/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 5/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 6/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 7/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 8/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 9/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 10/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 11/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 12/13] 40 tickers…\n",
      "  -> 201,320 lignes ajoutées\n",
      "[Batch 13/13] 23 tickers…\n",
      "  -> 115,759 lignes ajoutées\n",
      "\n",
      "✅ Fichier CSV : /home/onyxia/work/sp500_ohlcv_2015_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# pip install yfinance pandas requests\n",
    "\n",
    "import os, time, math\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "START = \"2005-01-01\"     # <- mets \"2005-01-01\" si tu veux plus long\n",
    "END   = \"2025-01-01\"\n",
    "CSV_OUT = \"sp500_ohlcv_2005_2025.csv\"\n",
    "BATCH = 40               # taille des lots (30–60 recommandé)\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 30\n",
    "\n",
    "def get_sp500_tickers() -> list[str]:\n",
    "    # 1) yfinance a une liste intégrée\n",
    "    try:\n",
    "        tickers = yf.tickers_sp500()\n",
    "        if tickers:\n",
    "            return [t.replace('.', '-') for t in tickers]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) fallback: Wikipédia avec User-Agent\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    html = requests.get(WIKI_URL, headers=headers, timeout=20).text\n",
    "    sp500 = pd.read_html(html, header=0)[0]\n",
    "    return sp500[\"Symbol\"].str.replace('.', '-', regex=False).tolist()\n",
    "\n",
    "def chunked(lst: List[str], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def download_batch(tickers: List[str]) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Télécharge un lot avec retries/backoff. Retourne (df_long, failed).\"\"\"\n",
    "    failed = list(tickers)\n",
    "    out_frames = []\n",
    "    for attempt in range(1, MAX_RETRIES+1):\n",
    "        if not failed:\n",
    "            break\n",
    "        try:\n",
    "            data = yf.download(\n",
    "                failed, start=START, end=END, interval=\"1d\",\n",
    "                group_by=\"ticker\", auto_adjust=True,\n",
    "                threads=False, progress=False, timeout=TIMEOUT\n",
    "            )\n",
    "        except Exception:\n",
    "            data = pd.DataFrame()\n",
    "\n",
    "        next_failed = []\n",
    "        for t in failed:\n",
    "            try:\n",
    "                # MultiIndex columns: niveau 0 = ticker\n",
    "                df_t = data[t].copy() if isinstance(data.columns, pd.MultiIndex) else data.copy()\n",
    "                if df_t.empty:\n",
    "                    next_failed.append(t)\n",
    "                    continue\n",
    "                df_t[\"Ticker\"] = t\n",
    "                out_frames.append(df_t.reset_index())\n",
    "            except Exception:\n",
    "                next_failed.append(t)\n",
    "\n",
    "        failed = next_failed\n",
    "        if failed and attempt < MAX_RETRIES:\n",
    "            time.sleep(3 * (2 ** (attempt-1)))  # backoff\n",
    "    df = pd.concat(out_frames, ignore_index=True) if out_frames else pd.DataFrame()\n",
    "    return df, failed\n",
    "\n",
    "def append_to_csv(df: pd.DataFrame, path: str):\n",
    "    # uniformise colonnes\n",
    "    keep = [c for c in [\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adj Close\"] if c in df.columns]\n",
    "    df = df[keep]\n",
    "    # si Adj Close == Close (auto_adjust=True), on peut drop\n",
    "    if \"Adj Close\" in df.columns and \"Close\" in df.columns and df[\"Adj Close\"].equals(df[\"Close\"]):\n",
    "        df = df.drop(columns=[\"Adj Close\"])\n",
    "    # écriture en mode append (header seulement si fichier absent)\n",
    "    header = not os.path.exists(path)\n",
    "    df.to_csv(path, mode=\"a\", index=False, header=header)\n",
    "\n",
    "def main():\n",
    "    tickers = get_sp500_tickers()\n",
    "    print(f\"{len(tickers)} tickers S&P 500 chargés. Exemples: {tickers[:10]}\")\n",
    "\n",
    "    if os.path.exists(CSV_OUT):\n",
    "        os.remove(CSV_OUT)\n",
    "\n",
    "    all_failed = []\n",
    "    total_batches = math.ceil(len(tickers)/BATCH)\n",
    "\n",
    "    for i, batch in enumerate(chunked(tickers, BATCH), 1):\n",
    "        print(f\"[Batch {i}/{total_batches}] {len(batch)} tickers…\")\n",
    "        df_batch, failed = download_batch(batch)\n",
    "        if not df_batch.empty:\n",
    "            append_to_csv(df_batch, CSV_OUT)\n",
    "            print(f\"  -> {len(df_batch):,} lignes ajoutées\")\n",
    "        if failed:\n",
    "            print(f\"  !! Échecs (tentatives groupées épuisées): {failed}\")\n",
    "            all_failed.extend(failed)\n",
    "\n",
    "    # Dernière chance: retenter 1 par 1 les échecs persistants\n",
    "    really_failed = []\n",
    "    if all_failed:\n",
    "        print(f\"Tentatives individuelles pour {len(all_failed)} tickers…\")\n",
    "        for t in all_failed:\n",
    "            df1, rem = download_batch([t])\n",
    "            if not df1.empty and not rem:\n",
    "                append_to_csv(df1, CSV_OUT)\n",
    "                print(f\"  -> récupéré {t}\")\n",
    "            else:\n",
    "                really_failed.append(t)\n",
    "\n",
    "    print(\"\\n✅ Fichier CSV :\", os.path.abspath(CSV_OUT))\n",
    "    if really_failed:\n",
    "        print(f\"⚠️ Impossible de télécharger après retries: {len(really_failed)} tickers\")\n",
    "        print(really_failed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2174db9-3ec0-4189-809c-10e28d684a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2531599 entries, 0 to 2531598\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   Date    object \n",
      " 1   Ticker  object \n",
      " 2   Open    float64\n",
      " 3   High    float64\n",
      " 4   Low     float64\n",
      " 5   Close   float64\n",
      " 6   Volume  float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 135.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sp500_ohlcv_2015_2025.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f772719c-a93f-479b-95e3-72da92217d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           0\n",
       "Ticker         0\n",
       "Open      227159\n",
       "High      227159\n",
       "Low       227159\n",
       "Close     227159\n",
       "Volume    227159\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40292d4a-1831-46af-87d6-066782098e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S'assurer que Date est bien au format datetime\n",
    "df = df.reset_index()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Filtrer pour garder seulement les dates >= 2015-01-01\n",
    "df = df[df[\"Date\"] >= \"2015-01-01\"]\n",
    "\n",
    "# Si tu veux reposer l'index comme avant (Date, Ticker)\n",
    "df = df.set_index([\"Date\", \"Ticker\"]).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09322841-7cdb-4c2b-9826-e726a3bcbc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index         0\n",
       "Open      40829\n",
       "High      40829\n",
       "Low       40829\n",
       "Close     40829\n",
       "Volume    40829\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff689b02-e05f-475e-9cf0-2c1bdf450d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
